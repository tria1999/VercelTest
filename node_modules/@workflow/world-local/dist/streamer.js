import { EventEmitter } from 'node:events';
import path from 'node:path';
import { monotonicFactory } from 'ulid';
import { z } from 'zod';
import { listJSONFiles, readBuffer, readJSON, write, writeJSON } from './fs.js';
// Create a monotonic ULID factory that ensures ULIDs are always increasing
// even when generated within the same millisecond
const monotonicUlid = monotonicFactory(() => Math.random());
// Schema for the run-to-streams mapping file
const RunStreamsSchema = z.object({
    streams: z.array(z.string()),
});
export function serializeChunk(chunk) {
    const eofByte = Buffer.from([chunk.eof ? 1 : 0]);
    return Buffer.concat([eofByte, chunk.chunk]);
}
export function deserializeChunk(serialized) {
    const eof = serialized[0] === 1;
    // Create a copy instead of a view to prevent ArrayBuffer detachment
    const chunk = Buffer.from(serialized.subarray(1));
    return { eof, chunk };
}
export function createStreamer(basedir) {
    const streamEmitter = new EventEmitter();
    // Track which streams have already been registered for a run (in-memory cache)
    const registeredStreams = new Set();
    // Helper to record the runId <> streamId association
    async function registerStreamForRun(runId, streamName) {
        const cacheKey = `${runId}:${streamName}`;
        if (registeredStreams.has(cacheKey)) {
            return; // Already registered in this session
        }
        const runStreamsPath = path.join(basedir, 'streams', 'runs', `${runId}.json`);
        // Read existing streams for this run
        const existing = await readJSON(runStreamsPath, RunStreamsSchema);
        const streams = existing?.streams ?? [];
        // Add stream if not already present
        if (!streams.includes(streamName)) {
            streams.push(streamName);
            await writeJSON(runStreamsPath, { streams }, { overwrite: true });
        }
        registeredStreams.add(cacheKey);
    }
    return {
        async writeToStream(name, _runId, chunk) {
            // Generate ULID synchronously BEFORE any await to preserve call order.
            // This ensures that chunks written in sequence maintain their order even
            // when runId is a promise that multiple writes are waiting on.
            const chunkId = `chnk_${monotonicUlid()}`;
            // Await runId if it's a promise to ensure proper flushing
            const runId = await _runId;
            // Register this stream for the run
            await registerStreamForRun(runId, name);
            // Convert chunk to buffer for serialization
            let chunkBuffer;
            if (typeof chunk === 'string') {
                chunkBuffer = Buffer.from(new TextEncoder().encode(chunk));
            }
            else if (chunk instanceof Buffer) {
                chunkBuffer = chunk;
            }
            else {
                chunkBuffer = Buffer.from(chunk);
            }
            const serialized = serializeChunk({
                chunk: chunkBuffer,
                eof: false,
            });
            const chunkPath = path.join(basedir, 'streams', 'chunks', `${name}-${chunkId}.json`);
            await write(chunkPath, serialized);
            // Emit real-time event with Uint8Array (create copy to prevent ArrayBuffer detachment)
            const chunkData = Uint8Array.from(chunkBuffer);
            streamEmitter.emit(`chunk:${name}`, {
                streamName: name,
                chunkData,
                chunkId,
            });
        },
        async closeStream(name, _runId) {
            // Generate ULID synchronously BEFORE any await to preserve call order.
            const chunkId = `chnk_${monotonicUlid()}`;
            // Await runId if it's a promise to ensure proper flushing
            const runId = await _runId;
            // Register this stream for the run (in case writeToStream wasn't called)
            await registerStreamForRun(runId, name);
            const chunkPath = path.join(basedir, 'streams', 'chunks', `${name}-${chunkId}.json`);
            await write(chunkPath, serializeChunk({ chunk: Buffer.from([]), eof: true }));
            streamEmitter.emit(`close:${name}`, { streamName: name });
        },
        async listStreamsByRunId(runId) {
            const runStreamsPath = path.join(basedir, 'streams', 'runs', `${runId}.json`);
            const data = await readJSON(runStreamsPath, RunStreamsSchema);
            return data?.streams ?? [];
        },
        async readFromStream(name, startIndex = 0) {
            const chunksDir = path.join(basedir, 'streams', 'chunks');
            let removeListeners = () => { };
            return new ReadableStream({
                async start(controller) {
                    // Track chunks delivered via events to prevent duplicates and maintain order.
                    const deliveredChunkIds = new Set();
                    // Buffer for chunks that arrive via events during disk reading
                    const bufferedEventChunks = [];
                    let isReadingFromDisk = true;
                    // Buffer close event if it arrives during disk reading
                    let pendingClose = false;
                    const chunkListener = (event) => {
                        deliveredChunkIds.add(event.chunkId);
                        // Skip empty chunks to maintain consistency with disk reading behavior
                        // Empty chunks are not enqueued when read from disk (see line 184-186)
                        if (event.chunkData.byteLength === 0) {
                            return;
                        }
                        if (isReadingFromDisk) {
                            // Buffer chunks that arrive during disk reading to maintain order
                            // Create a copy to prevent ArrayBuffer detachment when enqueued later
                            bufferedEventChunks.push({
                                chunkId: event.chunkId,
                                chunkData: Uint8Array.from(event.chunkData),
                            });
                        }
                        else {
                            // After disk reading is complete, deliver chunks immediately
                            // Create a copy to prevent ArrayBuffer detachment
                            controller.enqueue(Uint8Array.from(event.chunkData));
                        }
                    };
                    const closeListener = () => {
                        // Buffer close event if disk reading is still in progress
                        if (isReadingFromDisk) {
                            pendingClose = true;
                            return;
                        }
                        // Remove listeners before closing
                        streamEmitter.off(`chunk:${name}`, chunkListener);
                        streamEmitter.off(`close:${name}`, closeListener);
                        try {
                            controller.close();
                        }
                        catch {
                            // Ignore if controller is already closed (e.g., from cancel() or EOF)
                        }
                    };
                    removeListeners = closeListener;
                    // Set up listeners FIRST to avoid missing events
                    streamEmitter.on(`chunk:${name}`, chunkListener);
                    streamEmitter.on(`close:${name}`, closeListener);
                    // Now load existing chunks from disk
                    const files = await listJSONFiles(chunksDir);
                    const chunkFiles = files
                        .filter((file) => file.startsWith(`${name}-`))
                        .sort(); // ULID lexicographic sort = chronological order
                    // Process existing chunks, skipping any already delivered via events
                    let isComplete = false;
                    for (let i = startIndex; i < chunkFiles.length; i++) {
                        const file = chunkFiles[i];
                        // Extract chunk ID from filename: "streamName-chunkId"
                        const chunkId = file.substring(name.length + 1);
                        // Skip if already delivered via event
                        if (deliveredChunkIds.has(chunkId)) {
                            continue;
                        }
                        const chunk = deserializeChunk(await readBuffer(path.join(chunksDir, `${file}.json`)));
                        if (chunk?.eof === true) {
                            isComplete = true;
                            break;
                        }
                        if (chunk.chunk.byteLength) {
                            // Create a copy to prevent ArrayBuffer detachment
                            controller.enqueue(Uint8Array.from(chunk.chunk));
                        }
                    }
                    // Finished reading from disk - now deliver buffered event chunks in chronological order
                    isReadingFromDisk = false;
                    // Sort buffered chunks by ULID (chronological order)
                    bufferedEventChunks.sort((a, b) => a.chunkId.localeCompare(b.chunkId));
                    for (const buffered of bufferedEventChunks) {
                        // Create a copy for defense in depth (already copied at storage, but be extra safe)
                        controller.enqueue(Uint8Array.from(buffered.chunkData));
                    }
                    if (isComplete) {
                        removeListeners();
                        try {
                            controller.close();
                        }
                        catch {
                            // Ignore if controller is already closed (e.g., from closeListener event)
                        }
                        return;
                    }
                    // Process any pending close event that arrived during disk reading
                    if (pendingClose) {
                        streamEmitter.off(`chunk:${name}`, chunkListener);
                        streamEmitter.off(`close:${name}`, closeListener);
                        try {
                            controller.close();
                        }
                        catch {
                            // Ignore if controller is already closed
                        }
                    }
                },
                cancel() {
                    removeListeners();
                },
            });
        },
    };
}
//# sourceMappingURL=streamer.js.map